{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with torch.nn modules\n",
    "Implements everything that is asked in mini-project _with_ the use of the torch.nn functions. Specifically:\n",
    "* Generates a training and a test set of 1000 points sampled uniformly in [0,1]x[0,1] each with a label 0 if outside the disk of radius 1/sqrt(2*pi) and label 1 inside,\n",
    "* builds a network with two input units, two output units, three hidden layers of 25 units\n",
    "* trains it with MSE, logging the loss\n",
    "* computes and prints the final train and the test errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn  # not allowed in project 2; just for baseline comparison\n",
    "from torch.autograd import Variable\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb, one_hot=False):\n",
    "    \"\"\"return torch.FloatTensor of size nb x 2 and \n",
    "    torch.FloatTensor of size nb (if one_hot=False) or nb x 2 (if one_hot=True) \"\"\"\n",
    "    coordinates = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = coordinates.pow(2).sum(dim=1).sub(1.0/(2.0*pi)).sign().add(1).div(2).long() # 0.5* [sign(x^2 + y^2 - 2/pi) + 1]\n",
    "    if one_hot:\n",
    "        # Useful for MSE loss: convert from scalar labels to vector labels\n",
    "        target_one_hot = -torch.ones((nb, 2)).long()\n",
    "        # Stupid Tensor does not seem to have an efficient way to do this; call it quits and use ugly loop\n",
    "        for k in range(target.size(0)):\n",
    "            target_one_hot[k, target[k]] = 1\n",
    "        target = target_one_hot\n",
    "    return (coordinates, target)\n",
    "\n",
    "def check_and_normalize(train_input, train_target, test_input, test_target, one_hot=False):\n",
    "    \"\"\"Normalize train and test data by subtracting train mean and dividing by train standard deviation\"\"\"\n",
    "    if one_hot:\n",
    "        in_disk_train = (train_target[:, 1]==1).sum()\n",
    "        in_disk_test = (test_target[:, 1]==1).sum()\n",
    "    else:\n",
    "        in_disk_train = train_target.sum()\n",
    "        in_disk_test = test_target.sum()\n",
    "\n",
    "    print(\"Sanity check: {:d} out of {:d} training points inside disk, i.e.\"\n",
    "          \" {:3.2f}% (expected: 50%).\".format(in_disk_train, train_input.size(0), 100*in_disk_train/train_input.size(0)))\n",
    "    print(\"Sanity check: {:d} out of {:d} test points inside disk, i.e.\"\n",
    "          \" {:3.2f}% (expected: 50%).\".format(in_disk_test, test_input.size(0), 100*in_disk_test/test_input.size(0)))\n",
    "\n",
    "    # Normalize train and test input: subtract training mean and divide by training variance\n",
    "    train_mean, train_std = train_input.mean(0), train_input.std(0)\n",
    "\n",
    "    train_input.sub_(train_mean).div_(train_std)\n",
    "    test_input.sub_(train_mean).div_(train_std)\n",
    "\n",
    "    print(\"Sanity check: mean\", \" \".join(\"%.4g\" % (i,) for i in train_input.mean(0)), \" (expected 0 0)\"\n",
    "          \" and variance:\", \" \".join(\"%.3g\" % (i,) for i in train_input.std(0)), \"(expected 1 1).\")\n",
    "\n",
    "    return Variable(train_input), Variable(train_target), Variable(test_input), Variable(test_target)\n",
    "\n",
    "def train_model(model, train_input, train_target, eta=0.1, n_epochs=250, batch_size=100, momentum=0.8, log_loss=False):\n",
    "    Ndata = train_input.size(0)\n",
    "    # loss_function = torch.nn.CrossEntropyLoss() # takes 2 args: network output and sample target\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=eta, momentum=momentum) # SOL\n",
    "    for e in range(0, n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for b_start in range(0, train_input.size(0), batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start + batch_size - Ndata)   # accounts for boundary effects\n",
    "            batch_output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = loss_function(batch_output, train_target.narrow(0, b_start, bsize_eff).float())  # instance of Variable\n",
    "            epoch_loss = epoch_loss + batch_loss.data[0]\n",
    "            model.zero_grad()  # seems to work well outside of batch loop, sort of inertia?\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        if log_loss and e % round(n_epochs/4) == 0:  # prints 4 times\n",
    "            print(\"Epoch {:d}/{:d}: epoch loss {:5.4g}\".format(e+1, n_epochs, epoch_loss))\n",
    "\n",
    "def compute_nb_errors(model, data_input, data_target, one_hot=False, batch_size=100):\n",
    "    nb_errors = 0\n",
    "    nb_good = 0\n",
    "    Ndata = data_input.size(0)\n",
    "    for b_start in range(0, data_input.size(0), batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start + batch_size - Ndata)   # accounts for boundary effects\n",
    "        batch_output = model(data_input.narrow(0, b_start, bsize_eff))  # Nbatch x 2 if one_hot=True, Nbatch otherwise\n",
    "        if one_hot:\n",
    "            pred_label = batch_output.max(dim=1)[1]  # size Nbatch\n",
    "            data_label = data_target.narrow(0, b_start, bsize_eff).max(dim=1)[1]  # could be done outside the batch loop; size is Nbatch\n",
    "            nb_err_batch = 0\n",
    "            for k in range(bsize_eff): # not very efficient but safest bet given how poorly torch operations are designed\n",
    "                if data_label.data[k] != pred_label.data[k]: # data extracts torch.Tensor out of Variable\n",
    "                    nb_err_batch = nb_err_batch + 1\n",
    "        else:\n",
    "            nb_err_batch = (batch_output.max(1)[1] != data_target.narrow(0, b_start, bsize_eff)).long().sum()\n",
    "        # HUGE overflow problem if conversion to Long Int not performed, treated as short 1-byte int otherwise!!\n",
    "        nb_errors += nb_err_batch\n",
    "    if isinstance(nb_errors, torch.autograd.Variable):\n",
    "        nb_errors = nb_errors.data[0]\n",
    "    return nb_errors\n",
    "\n",
    "def create_miniproject2_model(nonlin_activ=nn.ReLU()):\n",
    "    return nn.Sequential(nn.Linear(2, 25), nonlin_activ, nn.Linear(25, 25), nonlin_activ, nn.Linear(25, 25), nonlin_activ, nn.Linear(25, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: 889 out of 1000 training points inside disk, i.e. 88.90% (expected: 50%).\n",
      "Sanity check: 4353 out of 5000 test points inside disk, i.e. 87.06% (expected: 50%).\n",
      "Sanity check: mean 3.147e-07 -7.411e-07  (expected 0 0) and variance: 1 1 (expected 1 1).\n"
     ]
    }
   ],
   "source": [
    "# Generate toy-example data\n",
    "nb_train = 1000\n",
    "nb_test = 5000\n",
    "\n",
    "# Set one_hot=True to generate targets with C dimensions, C being the number of classes (C=2 here) s.t. \n",
    "# target[i]=1 when i=C and target[i]=-1 elsewhere, for i=1,..., nb\n",
    "# one_hot=True is necessary for MSELoss; one_hot=False is necessary for torch.nn.CrossEntropyLoss()\n",
    "one_hot = True  \n",
    "\n",
    "train_input, train_target = generate_disc_set(nb_train, one_hot)\n",
    "test_input, test_target = generate_disc_set(nb_test, one_hot)\n",
    "\n",
    "(train_input,\n",
    " train_target,\n",
    " test_input,\n",
    " test_target) = check_and_normalize(train_input, train_target, test_input, test_target, one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideal network for comparison\n",
    "- dropout can certainly help! See lecture's toy example (same as ours!), handout 6, slide 44/82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch default initialization:train error 0.3%, test error 0.28%\n",
      "\n",
      "Gaussian initialization N(0,sig=0.001): train error 11.1%, test error 12.94%\n",
      "Uniform initialization [-0.001,0.001]: train error 11.1%, test error 12.94%\n",
      "\n",
      "Gaussian initialization N(0,sig=0.01): train error 2.3%, test error 2.3%\n",
      "Uniform initialization [-0.01,0.01]: train error 11.1%, test error 12.94%\n",
      "\n",
      "Gaussian initialization N(0,sig=0.1): train error 0.3%, test error 0.32%\n",
      "Uniform initialization [-0.1,0.1]: train error 0.4%, test error 0.42%\n",
      "\n",
      "Gaussian initialization N(0,sig=1): train error 0%, test error 0.32%\n",
      "Uniform initialization [-1,1]: train error 0.2%, test error 0.2%\n",
      "\n",
      "Gaussian initialization N(0,sig=10): train error 5.2%, test error 4.48%\n",
      "Uniform initialization [-10,10]: train error 3.9%, test error 3.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meta parameters\n",
    "log_loss = True  # True for printing the loss during the training; False for no verbose at all\n",
    "batch_size = 150  # does not have to divide total number of training sample but it's probably better to do so\n",
    "n_epochs = 250  # number of times the training samples are visited\n",
    "eta = 0.1  # learning rate\n",
    "momentum = 0.8  # \"inertia\" (see handout 5, slide 22/83) \n",
    "non_lin_activation = nn.Tanh()  # nn.Tanh(), nn.ReLU(), ... (http://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "model = create_miniproject2_model(non_lin_activation)\n",
    "\n",
    "# Train and test 3-hidden-layer MLP with various initialization strategies\n",
    "for std in [\"PyTorch\", 1e-3, 1e-2, 1e-1, 1, 10]:\n",
    "    if type(std) is str: \n",
    "        # PyTorch default initialization (supposed to be good)\n",
    "        train_model(model, train_input, train_target, n_epochs=n_epochs, eta=eta, batch_size=batch_size, log_loss=log_loss)\n",
    "        n_err_train = compute_nb_errors(model, train_input, train_target, one_hot, batch_size)\n",
    "        n_err_test = compute_nb_errors(model, test_input, test_target, one_hot, batch_size)\n",
    "        print(\"PyTorch default initialization:train error {:g}%, test error {:g}%\\n\".format(\n",
    "            n_err_train*100/nb_train, n_err_test*100/nb_test))  \n",
    "    else:\n",
    "        # Gaussian initialization\n",
    "        for p in model.parameters():\n",
    "            p.data.normal_(0, std)\n",
    "        train_model(model, train_input, train_target, eta=eta, n_epochs=n_epochs, batch_size=batch_size, log_loss=log_loss)\n",
    "        n_err_train = compute_nb_errors(model, train_input, train_target, one_hot, batch_size)\n",
    "        n_err_test = compute_nb_errors(model, test_input, test_target, one_hot, batch_size)\n",
    "        print(\"Gaussian initialization N(0,sig={}): train error {:g}%, test error {:g}%\".format(\n",
    "            std, n_err_train*100/nb_train, n_err_test*100/nb_test))\n",
    "        \n",
    "        # Uniform initialization (avoid repeating PyTorch default initialization)\n",
    "        for p in model.parameters():\n",
    "            p.data.uniform_(-std, std)\n",
    "        train_model(model, train_input, train_target, eta=eta, n_epochs=n_epochs, batch_size=batch_size, log_loss=log_loss)\n",
    "        n_err_train = compute_nb_errors(model, train_input, train_target, one_hot, batch_size)\n",
    "        n_err_test = compute_nb_errors(model, test_input, test_target, one_hot, batch_size)\n",
    "        print(\"Uniform initialization [{},{}]: train error {:g}%, test error {:g}%\\n\".format(\n",
    "            -std, std, n_err_train*100/nb_train, n_err_test*100/nb_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (deeplearningepfl)",
   "language": "python",
   "name": "deeplearningepfl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
