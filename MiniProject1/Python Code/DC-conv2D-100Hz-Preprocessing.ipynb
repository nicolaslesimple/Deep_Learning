{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input 100 Hz: 316x28x50\n",
      "Train target 100 Hz: 316\n",
      "Test input 100 Hz: 100x28x50\n",
      "Test target 100 Hz: 100\n",
      "\n",
      "Train input 1000 Hz: 316x28x500\n",
      "Train target 1000 Hz: 316\n",
      "Test input 1000 Hz: 100x28x500\n",
      "Test target 1000 Hz: 100\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci\n",
    "\n",
    "train_input_100 , train_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False)\n",
    "test_input_100 , test_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False, train = False)\n",
    "\n",
    "train_input_1000 , train_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, one_khz = True)\n",
    "test_input_1000 , test_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, train = False, one_khz = True)\n",
    "\n",
    "print(\"Train input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_100.size())))\n",
    "print(\"Train target 100 Hz: {:d}\".format(*(s for s in train_target_100.size())))\n",
    "print(\"Test input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_100.size())))\n",
    "print(\"Test target 100 Hz: {:d}\".format(*(s for s in test_target_100.size())))\n",
    "print(\"\")\n",
    "print(\"Train input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_1000.size())))\n",
    "print(\"Train target 1000 Hz: {:d}\".format(*(s for s in train_target_1000.size())))\n",
    "print(\"Test input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_1000.size())))\n",
    "print(\"Test target 1000 Hz: {:d}\".format(*(s for s in test_target_1000.size())))\n",
    "\n",
    "Ntrain = train_input_100.size(0)\n",
    "Ntest = test_input_100.size(0)\n",
    "Nchannels = train_input_100.size(1)\n",
    "Nsamples_100 = train_input_100.size(-1)\n",
    "Nsamples_1000 = train_input_1000.size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, Nchannels, Nsamples, output_units):\n",
    "        \"\"\"Initializes neural network with 3 convolutional layers and 1 fully-connected layer.\n",
    "        \n",
    "        Args:\n",
    "            - Nchannels (int): number of EEG channels\n",
    "            - Nsamples (int): number of time points in each EEG signal\n",
    "            - output_units (int): number of output units, e.g. 1 for training with loss torch.nn.BCELoss or 2 with \n",
    "            loss torch.nn.CrossEntropyLoss            \n",
    "            \n",
    "            \"\"\"\n",
    "        super(conv2DNet, self).__init__()\n",
    "        # Layer 1\n",
    "        l1_channels = 16  \n",
    "        self.conv1 = nn.Conv2d(1, l1_channels, (Nchannels, 1), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(l1_channels, False) # final size bsize x 1 x l1_channels x Nsamples\n",
    "\n",
    "        # Layer 2\n",
    "        l2_channels = 4\n",
    "        l2_temp_window = 32\n",
    "        l2_l1channel_overlap = 2\n",
    "        self.padding1 = nn.ZeroPad2d((l2_temp_window // 2, l2_temp_window // 2 - 1, l2_l1channel_overlap//2-1, l2_l1channel_overlap//2)) # left, right, top, bottom\n",
    "        self.conv2 = nn.Conv2d(1, l2_channels, (l2_l1channel_overlap, l2_temp_window))  # does not change size if combined with above padding\n",
    "        self.batchnorm2 = nn.BatchNorm2d(l2_channels, False)\n",
    "        self.pooling2 = nn.MaxPool2d((2, 4)) # final size bsize x l2_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "\n",
    "        # Layer 3\n",
    "        l3_channels = 4\n",
    "        l3_temp_window = 4\n",
    "        l3_l2channel_overlap = 8\n",
    "        self.padding2 = nn.ZeroPad2d((l3_temp_window//2, l3_temp_window//2-1, l3_l2channel_overlap//2, l3_l2channel_overlap//2-1))\n",
    "        self.conv3 = nn.Conv2d(l2_channels, l3_channels, (l3_l2channel_overlap, l3_temp_window))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(l3_channels, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4)) # final size bsize x l3_channels x floor(l1_channels/4) x floor(Nsamples/16)\n",
    "\n",
    "        # FC Layer\n",
    "        fc_inputs = l3_channels * (l1_channels//4) * (Nsamples//16)\n",
    "        print('fc_inputs', fc_inputs)\n",
    "        self.fc1 = nn.Linear(fc_inputs, output_units)\n",
    "        #self.fc2 = nn.Linear(24, 6)\n",
    "        #self.fc3 = nn.Linear(6, output_units)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies forward pass consisting of 3 convolutional layers followed by a fully-connected linear layer.\n",
    "        \n",
    "        Args:\n",
    "            - x (torch.autograd.Variable): the input batch. It has dimension batch_size x Nchannel x Nsamples x 1,\n",
    "            where Nchannel is the number of EEG channels and Nsamples the number of time points.\n",
    "        \n",
    "        Returns:\n",
    "            - (torch.autograd.Variable) of size either batch_size x output_units   \n",
    "        \n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 1, 2)             # bsize x 1 x Nchannels x Nsamples\n",
    "        \n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))              # bsize x l1_channels x 1 x Nsamples\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout2d(x, 0.3)\n",
    "        x = x.permute(0, 2, 1, 3)             # bsize x 1 x l1_channels x Nsamples\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))              # bsize x l2_channels x l1_channels x Nsamples\n",
    "        x = self.batchnorm2(x)       \n",
    "        x = F.dropout2d(x, 0.3)\n",
    "        x = self.pooling2(x)                  # bsize x l2_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))              # bsize x l3_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout2d(x, 0.3)\n",
    "        x = self.pooling3(x)                  # bsize x l3_channels x floor(l1_channels/4) x floor(Nsamples/16)\n",
    "\n",
    "        # Fully-connected Layer\n",
    "        x = x.view(-1, self.fc1.in_features)  # bsize x (l3_channels*floor(l1_channels/4)*floor(Nsamples/16))\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        x = F.sigmoid(self.fc1(x))            # bisze x self.fc1.out_features  \n",
    "        \n",
    "        if self.fc1.out_features == 1:\n",
    "            x = x.view(-1)                     # bsize (1D if 1 output unit)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, batch_size):\n",
    "    nb_errors = 0\n",
    "    Ndata = data_input.size(0)\n",
    "    model.eval()\n",
    "    \n",
    "    for b_start in range(0, data_input.size(0), batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start+batch_size-Ndata)  # boundary case\n",
    "        batch_output = model.forward(data_input.narrow(0, b_start, bsize_eff))  # is Variable if data_input is Variable\n",
    "        if len(list(batch_output.size()))>1 and batch_output.size(1) > 1:\n",
    "            # as many ouputs as there are classes => select maximum output\n",
    "            nb_err_batch = (batch_output.max(1)[1] != data_target.narrow(0, b_start, bsize_eff)).long().sum()\n",
    "            # overflow problem if conversion to Long Int not performed, treated as short 1-byte int otherwise!!\n",
    "        else:\n",
    "            # output is a scalar in [0, 1]\n",
    "            nb_err_batch = batch_output.round().sub(data_target.narrow(0, b_start, bsize_eff)).sign().abs().sum()\n",
    "        \n",
    "        nb_errors += nb_err_batch\n",
    "    if isinstance(nb_errors, Variable):\n",
    "        nb_errors = nb_errors.data[0]\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3000, 28, 50)\n",
      "test (100, 28, 50)\n",
      "validation (160, 28, 50)\n",
      "Ntrain 3000\n",
      "Ntest 100\n",
      "Nvalidation 160\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train_100(train_input_1000, train_target_1000, True, True)\n",
    "preprocessed_input_test = preprocessing_test_100(test_input_100, True)\n",
    "\n",
    "#Remove Noise\n",
    "#preprocessed_input_train = denoisedSignals(preprocessed_input_train)\n",
    "#preprocessed_input_validation = denoisedSignals(preprocessed_input_validation)\n",
    "#preprocessed_input_test = denoisedSignals(preprocessed_input_test)\n",
    "#add random noise\n",
    "#preprocessed_input_train = whiteNoise(preprocessed_input_train)\n",
    "#preprocessed_input_validation = whiteNoise(preprocessed_input_validation)\n",
    "#preprocessed_input_test = whiteNoise(preprocessed_input_test)\n",
    "\n",
    "print('train', preprocessed_input_train.shape)\n",
    "print('test', preprocessed_input_test.shape)\n",
    "print('validation', preprocessed_input_validation.shape)\n",
    "\n",
    "labels_train = torch.from_numpy(preprocessed_input_train_target)\n",
    "labels_test = test_target_100\n",
    "labels_validation = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train).float()\n",
    "preprocessed_input_test = torch.from_numpy(preprocessed_input_test).float()\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation).float()\n",
    "\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:,0,0])\n",
    "Ntest = len(preprocessed_input_test[:,0,0])\n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0])\n",
    "\n",
    "print('Ntrain', Ntrain)\n",
    "print('Ntest', Ntest)\n",
    "print('Nvalidation', Nvalidation)\n",
    "\n",
    "train_input = Variable(preprocessed_input_train.view(Ntrain, Nchannels, Nsamples_100, 1))\n",
    "validation_input = Variable(preprocessed_input_validation.view(Nvalidation, Nchannels, Nsamples_100, 1), volatile=True )\n",
    "test_input = Variable(preprocessed_input_test.view(Ntest, Nchannels, Nsamples_100, 1), volatile=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training and testing\n",
    "Non-linearity: elu  \n",
    "\n",
    "\n",
    "|criterion | optimizer | lr  | momentum | batch size | Nepochs | Train acc. | Test acc.|\n",
    "|----------|-----------|-----|----------|------------|---------|------------|----------|\n",
    "| BCE  | Adam  |1e-1 | def. | 15 | 150 | 86.4 | 61.4 | \n",
    "| BCE  | Adam  |1e-1 | def. | 20 | 150 | 99.8 | 79.5 | \n",
    "| BCE  | SGD   | 1e-2 | 0.85 | 20 | 150 | 98.9  | 61.5 | \n",
    "| CE   | Adam  | 1e-2 | def. | 20 | 150 | 98.4  |  70.5 | \n",
    "| CE   | SGD   | 1e-2 | 0.85 | 20 | 150 | 99.1 | 75.1 |\n",
    "\n",
    "\n",
    "Non-linearity: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_inputs 48\n",
      "Epoch Number :  0\n",
      "\t Training accuracy:  77.6\n",
      "\t Validation accuracy  68.75\n",
      "\t Test accuracy  69.0\n",
      "\t Epoch Loss  125.60784149169922\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  87.3\n",
      "\t Validation accuracy  72.5\n",
      "\t Test accuracy  71.0\n",
      "\t Epoch Loss  85.52024841308594\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  94.73333333333333\n",
      "\t Validation accuracy  66.25\n",
      "\t Test accuracy  69.0\n",
      "\t Epoch Loss  54.234249114990234\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  97.6\n",
      "\t Validation accuracy  61.25\n",
      "\t Test accuracy  69.0\n",
      "\t Epoch Loss  31.739063262939453\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  98.7\n",
      "\t Validation accuracy  66.875\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  18.428911209106445\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  99.33333333333333\n",
      "\t Validation accuracy  68.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  10.487039566040039\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  99.7\n",
      "\t Validation accuracy  68.75\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  6.2089056968688965\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  99.93333333333334\n",
      "\t Validation accuracy  63.125\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  3.8110086917877197\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  99.96666666666667\n",
      "\t Validation accuracy  61.875\n",
      "\t Test accuracy  70.0\n",
      "\t Epoch Loss  2.4127745628356934\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  99.96666666666667\n",
      "\t Validation accuracy  60.0\n",
      "\t Test accuracy  69.0\n",
      "\t Epoch Loss  1.631267786026001\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  99.96666666666667\n",
      "\t Validation accuracy  56.25\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  1.114738941192627\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  99.96666666666667\n",
      "\t Validation accuracy  55.0\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.796448826789856\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  99.96666666666667\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  0.5944238901138306\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  0.46330830454826355\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.370832622051239\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.30121544003486633\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.2471761852502823\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.20459695160388947\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.17128360271453857\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.14414294064044952\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.12202200293540955\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.1040865033864975\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.08906533569097519\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  68.0\n",
      "\t Epoch Loss  0.07662372291088104\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  0.06598491966724396\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  0.05729876086115837\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  67.0\n",
      "\t Epoch Loss  0.04955853894352913\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.043008964508771896\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.037329524755477905\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.032472532242536545\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.028340870514512062\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.024733463302254677\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.021619398146867752\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.018951809033751488\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.016654163599014282\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.014633115381002426\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.01289371307939291\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.011357737705111504\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.010027565062046051\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  0.008886684663593769\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.007841325365006924\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.006957285106182098\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.006171027664095163\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0054722316563129425\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.004854748025536537\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.004321818705648184\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.003824830986559391\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0034087570384144783\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0030286703258752823\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0026957790832966566\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00239554257132113\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.002129931468516588\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0018990233074873686\n",
      "Epoch Number :  53\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0016908199759200215\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0015066141495481133\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  53.75\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.001340876566246152\n",
      "Epoch Number :  56\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0011965326266363263\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0010654466459527612\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0009493801044300199\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0008448982844129205\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0007540224469266832\n",
      "Epoch Number :  61\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0006693443865515292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  62\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0005952204810455441\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  54.375\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0005311598652042449\n",
      "Epoch Number :  64\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  55.625\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0004746372578665614\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  56.25\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0004229801706969738\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  56.25\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00037643028190359473\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  56.875\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0003365750890225172\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  56.875\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0002998151758220047\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0002673679846338928\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00023858172062318772\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00021323369583114982\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00019054517906624824\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00017042052058968693\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00015263186651282012\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00013628842134494334\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.00012231682194396853\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  0.0001093948376365006\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  9.75378425209783e-05\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  8.757918112678453e-05\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  7.827308581909165e-05\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  6.997712625889108e-05\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  6.261713861022145e-05\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  5.6309196224901825e-05\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  5.0261602154932916e-05\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  4.500856448430568e-05\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  4.0358972910325974e-05\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  3.623094016802497e-05\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  3.237391865695827e-05\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  2.90842326648999e-05\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  2.6152003556489944e-05\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  2.330022834939882e-05\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  2.0903027689200826e-05\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  1.8758046280709095e-05\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  57.5\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  1.6875865185284056e-05\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  1.4988328075560275e-05\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  1.3542420674639288e-05\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  65.0\n",
      "\t Epoch Loss  1.2078405234206002e-05\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  1.0756164556369185e-05\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  100.0\n",
      "\t Validation accuracy  58.125\n",
      "\t Test accuracy  66.0\n",
      "\t Epoch Loss  9.729435078043025e-06\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Train network \n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss() #interesting ... but does not converge\n",
    "#criterion = nn.MSELoss() #0.83 but unstable\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(preprocessed_input_train_target.float()) # convert to float\n",
    "    validation_target = Variable(preprocessed_input_validation_target.float(), volatile=True ) # convert to float\n",
    "    test_target = Variable(test_target_100.float(), volatile=True )\n",
    "    Noutputs = 1\n",
    "        \n",
    "model = conv2DNet(Nchannels, Nsamples_100, Noutputs)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.90, nesterov=False)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#optimizer = optim.Adagrad(model.parameters())\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "#optimizer = optim.ASGD(model.parameters())\n",
    "#optimizer = optim.RMSprop(model.parameters())\n",
    "#optimizer = optim.Rprop(model.parameters())\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "batch_size = 15\n",
    "Nbatches = int(math.ceil(Ntrain/batch_size))\n",
    "Nepochs = 100\n",
    "Nrep = 1\n",
    "\n",
    "train_errors = torch.Tensor(Nepochs).zero_()\n",
    "test_errors = torch.Tensor(Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "ep_loss = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "for i_rep in range(Nrep):\n",
    "    for i_ep in range(Nepochs):\n",
    "        for b_start in range(0, Ntrain, batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))            \n",
    "            ep_loss[i_ep] += batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(ep_loss[i_ep])\n",
    "        \n",
    "        nb_train_errs = compute_nb_errors(model, train_input, train_target, batch_size)\n",
    "        nb_validation_errs = compute_nb_errors(model, validation_input, validation_target, batch_size)\n",
    "        nb_test_errs = compute_nb_errors(model, test_input, test_target, batch_size)\n",
    "        \n",
    "        print(\"Epoch Number : \", i_ep)\n",
    "        print(\"\\t Training accuracy: \", (100*(Ntrain-nb_train_errs)/Ntrain))\n",
    "        print(\"\\t Validation accuracy \",(100*(Nvalidation-nb_validation_errs)/Nvalidation)) \n",
    "        print(\"\\t Test accuracy \",(100*(Ntest-nb_test_errs)/Ntest))\n",
    "        \n",
    "        print(\"\\t Epoch Loss \", ep_loss[i_ep])\n",
    "        \n",
    "        train_errors[i_ep] = nb_train_errs\n",
    "        test_errors[i_ep] = nb_test_errs\n",
    "        validation_errors[i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_errors))\n",
    "plt.plot(np.array(validation_errors))\n",
    "plt.plot(np.array(test_errors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(100*(Ntrain-np.array(train_errors))/Ntrain)\n",
    "plt.plot(100*(Nvalidation-np.array(validation_errors))/Nvalidation)\n",
    "plt.plot(100*(Ntest-np.array(test_errors))/Ntest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
