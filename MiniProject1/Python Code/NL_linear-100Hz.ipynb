{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utility import *\n",
    "np.random.seed(237699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input 100 Hz: 316x28x50\n",
      "Train target 100 Hz: 316\n",
      "Test input 100 Hz: 100x28x50\n",
      "Test target 100 Hz: 100\n",
      "\n",
      "Train input 1000 Hz: 316x28x500\n",
      "Train target 1000 Hz: 316\n",
      "Test input 1000 Hz: 100x28x500\n",
      "Test target 1000 Hz: 100\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci\n",
    "\n",
    "train_input_100 , train_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False)\n",
    "test_input_100 , test_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False, train = False)\n",
    "\n",
    "train_input_1000 , train_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, one_khz = True)\n",
    "test_input_1000 , test_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, train = False, one_khz = True)\n",
    "\n",
    "print(\"Train input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_100.size())))\n",
    "print(\"Train target 100 Hz: {:d}\".format(*(s for s in train_target_100.size())))\n",
    "print(\"Test input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_100.size())))\n",
    "print(\"Test target 100 Hz: {:d}\".format(*(s for s in test_target_100.size())))\n",
    "print(\"\")\n",
    "print(\"Train input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_1000.size())))\n",
    "print(\"Train target 1000 Hz: {:d}\".format(*(s for s in train_target_1000.size())))\n",
    "print(\"Test input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_1000.size())))\n",
    "print(\"Test target 1000 Hz: {:d}\".format(*(s for s in test_target_1000.size())))\n",
    "\n",
    "Ntrain = train_input_100.size(0)\n",
    "Ntest = test_input_100.size(0)\n",
    "Nchannels = train_input_100.size(1)\n",
    "Nsamples_100 = train_input_100.size(-1)\n",
    "Nsamples_1000 = train_input_1000.size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*50,256)    \n",
    "        self.fc2 = nn.Linear(256,512)\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.fc4 = nn.Linear(256,64)\n",
    "        self.fc5 = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = x.view(-1,28*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x,0.8)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x,0.3)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x,0.3)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.dropout(x,0.8)      \n",
    "        x = F.softmax(self.fc5(x))\n",
    "        if self.fc5.out_features==1:\n",
    "            x=x.view(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet2, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*50,256)    \n",
    "        self.fc2 = nn.Linear(256,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.view(-1,28*50)\n",
    "        x = F.relu(self.fc1(x))     \n",
    "        x = F.softmax(self.fc2(x))\n",
    "        if self.fc2.out_features==1:\n",
    "            x=x.view(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet3, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*50,128)    \n",
    "        self.fc2 = nn.Linear(128,256)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(128,64)\n",
    "        self.fc5 = nn.Linear(64,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = x.view(-1,28*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = F.relu(self.fc4(x)) \n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        if self.fc5.out_features==1:\n",
    "            x=x.view(-1)       \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, batch_size):\n",
    "    nb_errors = 0\n",
    "    Ndata = data_input.size(0)\n",
    "    model.eval()\n",
    "    \n",
    "    for b_start in range(0, data_input.size(0), batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start+batch_size-Ndata)  # boundary case\n",
    "        batch_output = model.forward(data_input.narrow(0, b_start, bsize_eff))  # is Variable if data_input is Variable\n",
    "        if len(list(batch_output.size()))>1 and batch_output.size(1) > 1:\n",
    "            # as many ouputs as there are classes => select maximum output\n",
    "            nb_err_batch = (batch_output.max(1)[1] != data_target.narrow(0, b_start, bsize_eff)).long().sum()\n",
    "            # overflow problem if conversion to Long Int not performed, treated as short 1-byte int otherwise!!\n",
    "        else:\n",
    "            # output is a scalar in [0, 1]\n",
    "            nb_err_batch = batch_output.round().sub(data_target.narrow(0, b_start, bsize_eff)).sign().abs().sum()\n",
    "        \n",
    "        nb_errors += nb_err_batch\n",
    "    if isinstance(nb_errors, Variable):\n",
    "        nb_errors = nb_errors.data[0]\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (3000, 28, 50)\n",
      "test (100, 28, 50)\n",
      "validation (160, 28, 50)\n",
      "Ntrain 3000\n",
      "Ntest 100\n",
      "Nvalidation 160\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train_100(train_input_1000, train_target_1000, False, False)\n",
    "preprocessed_input_test = preprocessing_test_100(test_input_100, False)\n",
    "\n",
    "#Remove Noise\n",
    "#preprocessed_input_train = denoisedSignals(preprocessed_input_train)\n",
    "#preprocessed_input_validation = denoisedSignals(preprocessed_input_validation)\n",
    "#preprocessed_input_test = denoisedSignals(preprocessed_input_test)\n",
    "#add random noise\n",
    "#preprocessed_input_train = whiteNoise(preprocessed_input_train)\n",
    "#preprocessed_input_validation = whiteNoise(preprocessed_input_validation)\n",
    "#preprocessed_input_test = whiteNoise(preprocessed_input_test)\n",
    "\n",
    "print('train', preprocessed_input_train.shape)\n",
    "print('test', preprocessed_input_test.shape)\n",
    "print('validation', preprocessed_input_validation.shape)\n",
    "\n",
    "labels_train = torch.from_numpy(preprocessed_input_train_target)\n",
    "labels_test = test_target_100\n",
    "labels_validation = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train).float()\n",
    "preprocessed_input_test = torch.from_numpy(preprocessed_input_test).float()\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation).float()\n",
    "\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:,0,0])\n",
    "Ntest = len(preprocessed_input_test[:,0,0])\n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0])\n",
    "\n",
    "print('Ntrain', Ntrain)\n",
    "print('Ntest', Ntest)\n",
    "print('Nvalidation', Nvalidation)\n",
    "\n",
    "train_input = Variable(preprocessed_input_train.view(Ntrain, 1, Nchannels, Nsamples_100))\n",
    "validation_input = Variable(preprocessed_input_validation.view(Nvalidation, 1, Nchannels, Nsamples_100), volatile=True )\n",
    "test_input = Variable(preprocessed_input_test.view(Ntest, 1, Nchannels, Nsamples_100), volatile=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training and testing\n",
    "Non-linearity: elu  \n",
    "\n",
    "\n",
    "|criterion | optimizer | lr  | momentum | batch size | Nepochs | Train acc. | Test acc.|\n",
    "|----------|-----------|-----|----------|------------|---------|------------|----------|\n",
    "\n",
    "Non-linearity: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  0\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  53\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  56\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  61\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  62\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  64\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  50.333333333333336\n",
      "\t Validation accuracy  37.5\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  49.66667175292969\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Train network \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.SmoothL1Loss() #interesting ... but does not converge\n",
    "#criterion = nn.MSELoss() #0.83 but unstable\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(preprocessed_input_train_target.float()) # convert to float\n",
    "    validation_target = Variable(preprocessed_input_validation_target.float(), volatile=True ) # convert to float\n",
    "    test_target = Variable(test_target_100.float(), volatile=True )\n",
    "    Noutputs = 1\n",
    "        \n",
    "model = MyNet()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.40)\n",
    "#optimizer = optim.Adam(model.parameters())\n",
    "#optimizer = optim.Adagrad(model.parameters())\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "#optimizer = optim.ASGD(model.parameters())\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "#optimizer = optim.Rprop(model.parameters())\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "batch_size = 15\n",
    "Nbatches = int(math.ceil(Ntrain/batch_size))\n",
    "Nepochs = 100\n",
    "Nrep = 1\n",
    "\n",
    "train_errors = torch.Tensor(Nepochs).zero_()\n",
    "test_errors = torch.Tensor(Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "ep_loss = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "for i_rep in range(Nrep):\n",
    "    for i_ep in range(Nepochs):\n",
    "        for b_start in range(0, Ntrain, batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))            \n",
    "            ep_loss[i_ep] += batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(ep_loss[i_ep])\n",
    "        \n",
    "        nb_train_errs = compute_nb_errors(model, train_input, train_target, batch_size)\n",
    "        nb_validation_errs = compute_nb_errors(model, validation_input, validation_target, batch_size)\n",
    "        nb_test_errs = compute_nb_errors(model, test_input, test_target, batch_size)\n",
    "        \n",
    "        print(\"Epoch Number : \", i_ep)\n",
    "        print(\"\\t Training accuracy: \", (100*(Ntrain-nb_train_errs)/Ntrain))\n",
    "        print(\"\\t Validation accuracy \",(100*(Nvalidation-nb_validation_errs)/Nvalidation)) \n",
    "        print(\"\\t Test accuracy \",(100*(Ntest-nb_test_errs)/Ntest))\n",
    "        \n",
    "        print(\"\\t Epoch Loss \", ep_loss[i_ep])\n",
    "        \n",
    "        train_errors[i_ep] = nb_train_errs\n",
    "        test_errors[i_ep] = nb_test_errs\n",
    "        validation_errors[i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyyTldZWqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195698eb668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_errors))\n",
    "plt.plot(np.array(validation_errors))\n",
    "plt.plot(np.array(test_errors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(100*(Ntrain-np.array(train_errors))/Ntrain)\n",
    "plt.plot(100*(Nvalidation-np.array(validation_errors))/Nvalidation)\n",
    "plt.plot(100*(Ntest-np.array(test_errors))/Ntest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
