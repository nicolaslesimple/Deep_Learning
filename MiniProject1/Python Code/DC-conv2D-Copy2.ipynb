{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn \n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input 100 Hz: 316x28x50\n",
      "Train target 100 Hz: 316\n",
      "Test input 100 Hz: 100x28x50\n",
      "Test target 100 Hz: 100\n",
      "\n",
      "Train input 1000 Hz: 316x28x500\n",
      "Train target 1000 Hz: 316\n",
      "Test input 1000 Hz: 100x28x500\n",
      "Test target 1000 Hz: 100\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci\n",
    "\n",
    "train_input_100 , train_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False)\n",
    "test_input_100 , test_target_100 = dlc_bci.load(root = './data_bci_100Hz', download = False, train = False)\n",
    "\n",
    "train_input_1000 , train_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, one_khz = True)\n",
    "test_input_1000 , test_target_1000 = dlc_bci.load(root = './data_bci_1000Hz', download = False, train = False, one_khz = True)\n",
    "\n",
    "print(\"Train input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_100.size())))\n",
    "print(\"Train target 100 Hz: {:d}\".format(*(s for s in train_target_100.size())))\n",
    "print(\"Test input 100 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_100.size())))\n",
    "print(\"Test target 100 Hz: {:d}\".format(*(s for s in test_target_100.size())))\n",
    "print(\"\")\n",
    "print(\"Train input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in train_input_1000.size())))\n",
    "print(\"Train target 1000 Hz: {:d}\".format(*(s for s in train_target_1000.size())))\n",
    "print(\"Test input 1000 Hz: {:d}x{:d}x{:d}\".format(*(s for s in test_input_1000.size())))\n",
    "print(\"Test target 1000 Hz: {:d}\".format(*(s for s in test_target_1000.size())))\n",
    "\n",
    "Ntrain = train_input_100.size(0)\n",
    "Ntest = test_input_100.size(0)\n",
    "Nchannels = train_input_100.size(1)\n",
    "Nsamples_100 = train_input_100.size(-1)\n",
    "Nsamples_1000 = train_input_1000.size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, Nchannels, Nsamples, output_units):\n",
    "        \"\"\"Initializes neural network with 3 convolutional layers and 1 fully-connected layer.\n",
    "        \n",
    "        Args:\n",
    "            - Nchannels (int): number of EEG channels\n",
    "            - Nsamples (int): number of time points in each EEG signal\n",
    "            - output_units (int): number of output units, e.g. 1 for training with loss torch.nn.BCELoss or 2 with \n",
    "            loss torch.nn.CrossEntropyLoss            \n",
    "            \n",
    "            \"\"\"\n",
    "        super(conv2DNet, self).__init__()\n",
    "        # Layer 1\n",
    "        l1_channels = 16  \n",
    "        self.conv1 = nn.Conv2d(1, l1_channels, (Nchannels, 1), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(l1_channels, False) # final size bsize x 1 x l1_channels x Nsamples\n",
    "\n",
    "        # Layer 2\n",
    "        l2_channels = 4\n",
    "        l2_temp_window = 32\n",
    "        l2_l1channel_overlap = 2\n",
    "        self.padding1 = nn.ZeroPad2d((l2_temp_window // 2, l2_temp_window // 2 - 1, l2_l1channel_overlap//2-1, l2_l1channel_overlap//2)) # left, right, top, bottom\n",
    "        self.conv2 = nn.Conv2d(1, l2_channels, (l2_l1channel_overlap, l2_temp_window))  # does not change size if combined with above padding\n",
    "        self.batchnorm2 = nn.BatchNorm2d(l2_channels, False)\n",
    "        self.pooling2 = nn.MaxPool2d((2, 4)) # final size bsize x l2_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "\n",
    "        # Layer 3\n",
    "        l3_channels = 4\n",
    "        l3_temp_window = 4\n",
    "        l3_l2channel_overlap = 8\n",
    "        self.padding2 = nn.ZeroPad2d((l3_temp_window//2, l3_temp_window//2-1, l3_l2channel_overlap//2, l3_l2channel_overlap//2-1))\n",
    "        self.conv3 = nn.Conv2d(l2_channels, l3_channels, (l3_l2channel_overlap, l3_temp_window))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(l3_channels, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4)) # final size bsize x l3_channels x floor(l1_channels/4) x floor(Nsamples/16)\n",
    "\n",
    "        # FC Layer\n",
    "        fc_inputs = l3_channels * (l1_channels//4) * (Nsamples//16)\n",
    "        print('fc_inputs', fc_inputs)\n",
    "        self.fc1 = nn.Linear(fc_inputs, output_units)\n",
    "        #self.fc2 = nn.Linear(24, 6)\n",
    "        #self.fc3 = nn.Linear(6, output_units)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies forward pass consisting of 3 convolutional layers followed by a fully-connected linear layer.\n",
    "        \n",
    "        Args:\n",
    "            - x (torch.autograd.Variable): the input batch. It has dimension batch_size x Nchannel x Nsamples x 1,\n",
    "            where Nchannel is the number of EEG channels and Nsamples the number of time points.\n",
    "        \n",
    "        Returns:\n",
    "            - (torch.autograd.Variable) of size either batch_size x output_units   \n",
    "        \n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 1, 2)             # bsize x 1 x Nchannels x Nsamples\n",
    "        \n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))              # bsize x l1_channels x 1 x Nsamples\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout2d(x, 0.7)\n",
    "        x = x.permute(0, 2, 1, 3)             # bsize x 1 x l1_channels x Nsamples\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))              # bsize x l2_channels x l1_channels x Nsamples\n",
    "        x = self.batchnorm2(x)       \n",
    "        x = F.dropout2d(x, 0.7)\n",
    "        x = self.pooling2(x)                  # bsize x l2_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))              # bsize x l3_channels x floor(l1_channels/2) x floor(Nsamples/4)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout2d(x, 0.7)\n",
    "        x = self.pooling3(x)                  # bsize x l3_channels x floor(l1_channels/4) x floor(Nsamples/16)\n",
    "\n",
    "        # Fully-connected Layer\n",
    "        x = x.view(-1, self.fc1.in_features)  # bsize x (l3_channels*floor(l1_channels/4)*floor(Nsamples/16))\n",
    "        #x = self.fc1(x)\n",
    "        #x = self.fc2(x)\n",
    "        x = F.sigmoid(self.fc1(x))            # bisze x self.fc1.out_features  \n",
    "        \n",
    "        if self.fc1.out_features == 1:\n",
    "            x = x.view(-1)                     # bsize (1D if 1 output unit)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, batch_size):\n",
    "    nb_errors = 0\n",
    "    Ndata = data_input.size(0)\n",
    "    model.eval()\n",
    "    \n",
    "    for b_start in range(0, data_input.size(0), batch_size):\n",
    "        bsize_eff = batch_size - max(0, b_start+batch_size-Ndata)  # boundary case\n",
    "        batch_output = model.forward(data_input.narrow(0, b_start, bsize_eff))  # is Variable if data_input is Variable\n",
    "        if len(list(batch_output.size()))>1 and batch_output.size(1) > 1:\n",
    "            # as many ouputs as there are classes => select maximum output\n",
    "            nb_err_batch = (batch_output.max(1)[1] != data_target.narrow(0, b_start, bsize_eff)).long().sum()\n",
    "            # overflow problem if conversion to Long Int not performed, treated as short 1-byte int otherwise!!\n",
    "        else:\n",
    "            # output is a scalar in [0, 1]\n",
    "            nb_err_batch = batch_output.round().sub(data_target.narrow(0, b_start, bsize_eff)).sign().abs().sum()\n",
    "        \n",
    "        nb_errors += nb_err_batch\n",
    "    if isinstance(nb_errors, Variable):\n",
    "        nb_errors = nb_errors.data[0]\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (2705, 28, 50)\n",
      "test (100, 28, 50)\n",
      "validation (144, 28, 50)\n",
      "Ntrain 2705\n",
      "Ntest 100\n",
      "Nvalidation 144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utility import * \n",
    "\n",
    "preprocessed_input_train, preprocessed_input_validation, preprocessed_input_train_target, preprocessed_input_validation_target = preprocessing_train(train_input_1000, train_target_1000, False, False)\n",
    "preprocessed_input_test = preprocessing_test(test_input_100, False)\n",
    "\n",
    "#Remove Noise\n",
    "#preprocessed_input_train = denoisedSignals(preprocessed_input_train)\n",
    "#preprocessed_input_validation = denoisedSignals(preprocessed_input_validation)\n",
    "#preprocessed_input_test = denoisedSignals(preprocessed_input_test)\n",
    "#add random noise\n",
    "#preprocessed_input_train = whiteNoise(preprocessed_input_train)\n",
    "#preprocessed_input_validation = whiteNoise(preprocessed_input_validation)\n",
    "#preprocessed_input_test = whiteNoise(preprocessed_input_test)\n",
    "\n",
    "print('train', preprocessed_input_train.shape)\n",
    "print('test', preprocessed_input_test.shape)\n",
    "print('validation', preprocessed_input_validation.shape)\n",
    "\n",
    "labels_train = torch.from_numpy(preprocessed_input_train_target)\n",
    "labels_test = test_target_100\n",
    "labels_validation = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "preprocessed_input_train = torch.from_numpy(preprocessed_input_train).float()\n",
    "preprocessed_input_test = torch.from_numpy(preprocessed_input_test).float()\n",
    "preprocessed_input_validation = torch.from_numpy(preprocessed_input_validation).float()\n",
    "\n",
    "preprocessed_input_train_target = torch.from_numpy(preprocessed_input_train_target)\n",
    "preprocessed_input_validation_target = torch.from_numpy(preprocessed_input_validation_target)\n",
    "\n",
    "Ntrain = len(preprocessed_input_train[:,0,0])\n",
    "Ntest = len(preprocessed_input_test[:,0,0])\n",
    "Nvalidation = len(preprocessed_input_validation[:,0,0])\n",
    "\n",
    "print('Ntrain', Ntrain)\n",
    "print('Ntest', Ntest)\n",
    "print('Nvalidation', Nvalidation)\n",
    "\n",
    "train_input = Variable(preprocessed_input_train.view(Ntrain, Nchannels, Nsamples_100, 1))\n",
    "validation_input = Variable(preprocessed_input_validation.view(Nvalidation, Nchannels, Nsamples_100, 1), volatile=True )\n",
    "test_input = Variable(preprocessed_input_test.view(Ntest, Nchannels, Nsamples_100, 1), volatile=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network training and testing\n",
    "Non-linearity: elu  \n",
    "\n",
    "\n",
    "|criterion | optimizer | lr  | momentum | batch size | Nepochs | Train acc. | Test acc.|\n",
    "|----------|-----------|-----|----------|------------|---------|------------|----------|\n",
    "| BCE  | Adam  |1e-1 | def. | 15 | 150 | 86.4 | 61.4 | \n",
    "| BCE  | Adam  |1e-1 | def. | 20 | 150 | 99.8 | 79.5 | \n",
    "| BCE  | SGD   | 1e-2 | 0.85 | 20 | 150 | 98.9  | 61.5 | \n",
    "| CE   | Adam  | 1e-2 | def. | 20 | 150 | 98.4  |  70.5 | \n",
    "| CE   | SGD   | 1e-2 | 0.85 | 20 | 150 | 99.1 | 75.1 |\n",
    "\n",
    "\n",
    "Non-linearity: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_inputs 48\n",
      "Epoch Number :  0\n",
      "\t Training accuracy:  66.83918669131238\n",
      "\t Validation accuracy  61.80555555555556\n",
      "\t Test accuracy  54.0\n",
      "\t Epoch Loss  119.61275482177734\n",
      "Epoch Number :  1\n",
      "\t Training accuracy:  79.26062846580406\n",
      "\t Validation accuracy  76.38888888888889\n",
      "\t Test accuracy  64.0\n",
      "\t Epoch Loss  103.89993286132812\n",
      "Epoch Number :  2\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  75.01516723632812\n",
      "Epoch Number :  3\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  47.80127716064453\n",
      "Epoch Number :  4\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  31.653623580932617\n",
      "Epoch Number :  5\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  17.812665939331055\n",
      "Epoch Number :  6\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  10.719095230102539\n",
      "Epoch Number :  7\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  6.227275371551514\n",
      "Epoch Number :  8\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  4.5850067138671875\n",
      "Epoch Number :  9\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  5.893723011016846\n",
      "Epoch Number :  10\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  1.8205599784851074\n",
      "Epoch Number :  11\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  1.3118482828140259\n",
      "Epoch Number :  12\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  1.6929453611373901\n",
      "Epoch Number :  13\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.8947449922561646\n",
      "Epoch Number :  14\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.634976863861084\n",
      "Epoch Number :  15\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.5302964448928833\n",
      "Epoch Number :  16\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.422677218914032\n",
      "Epoch Number :  17\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.2998206913471222\n",
      "Epoch Number :  18\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.2471417486667633\n",
      "Epoch Number :  19\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.20827780663967133\n",
      "Epoch Number :  20\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.1791609674692154\n",
      "Epoch Number :  21\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.1521032154560089\n",
      "Epoch Number :  22\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.13192985951900482\n",
      "Epoch Number :  23\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.1138494685292244\n",
      "Epoch Number :  24\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.09933602809906006\n",
      "Epoch Number :  25\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.08592059463262558\n",
      "Epoch Number :  26\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.07560453563928604\n",
      "Epoch Number :  27\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.06584982573986053\n",
      "Epoch Number :  28\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.05814880132675171\n",
      "Epoch Number :  29\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.051100920885801315\n",
      "Epoch Number :  30\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.045278314501047134\n",
      "Epoch Number :  31\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.03984644636511803\n",
      "Epoch Number :  32\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.035396676510572433\n",
      "Epoch Number :  33\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.031573936343193054\n",
      "Epoch Number :  34\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.027984097599983215\n",
      "Epoch Number :  35\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.02501901239156723\n",
      "Epoch Number :  36\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.022185930982232094\n",
      "Epoch Number :  37\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.019839808344841003\n",
      "Epoch Number :  38\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.01766098663210869\n",
      "Epoch Number :  39\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.01575855165719986\n",
      "Epoch Number :  40\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.014109853655099869\n",
      "Epoch Number :  41\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.012595285661518574\n",
      "Epoch Number :  42\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.011281795799732208\n",
      "Epoch Number :  43\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.01008401159197092\n",
      "Epoch Number :  44\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.009046378545463085\n",
      "Epoch Number :  45\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.008094952441751957\n",
      "Epoch Number :  46\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.007249736692756414\n",
      "Epoch Number :  47\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.006495794281363487\n",
      "Epoch Number :  48\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.005817858036607504\n",
      "Epoch Number :  49\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.005229134112596512\n",
      "Epoch Number :  50\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.004691410809755325\n",
      "Epoch Number :  51\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.004211928695440292\n",
      "Epoch Number :  52\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0037939418107271194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number :  53\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.003404419170692563\n",
      "Epoch Number :  54\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.003060444025322795\n",
      "Epoch Number :  55\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.002756190951913595\n",
      "Epoch Number :  56\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0024809204041957855\n",
      "Epoch Number :  57\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.002231686608865857\n",
      "Epoch Number :  58\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0020080446265637875\n",
      "Epoch Number :  59\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0018100099405273795\n",
      "Epoch Number :  60\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0016331207007169724\n",
      "Epoch Number :  61\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0014693565899506211\n",
      "Epoch Number :  62\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0013226136798039079\n",
      "Epoch Number :  63\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0011920342221856117\n",
      "Epoch Number :  64\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.001075711683370173\n",
      "Epoch Number :  65\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0009702946990728378\n",
      "Epoch Number :  66\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.000874911667779088\n",
      "Epoch Number :  67\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0007873177528381348\n",
      "Epoch Number :  68\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0007086849655024707\n",
      "Epoch Number :  69\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0006399682606570423\n",
      "Epoch Number :  70\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0005761015345342457\n",
      "Epoch Number :  71\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0005219129379838705\n",
      "Epoch Number :  72\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00047114246990531683\n",
      "Epoch Number :  73\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00042389542795717716\n",
      "Epoch Number :  74\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0003843673912342638\n",
      "Epoch Number :  75\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00034597894409671426\n",
      "Epoch Number :  76\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0003134895523544401\n",
      "Epoch Number :  77\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0002819810470100492\n",
      "Epoch Number :  78\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0002542449510656297\n",
      "Epoch Number :  79\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0002298610779689625\n",
      "Epoch Number :  80\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.0002076930832117796\n",
      "Epoch Number :  81\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00018669312703423202\n",
      "Epoch Number :  82\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00016875399160198867\n",
      "Epoch Number :  83\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00015289003204088658\n",
      "Epoch Number :  84\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00013732101069763303\n",
      "Epoch Number :  85\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00012441701255738735\n",
      "Epoch Number :  86\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00011192621605005115\n",
      "Epoch Number :  87\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  0.00010132328316103667\n",
      "Epoch Number :  88\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  9.106645302381366e-05\n",
      "Epoch Number :  89\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  8.265824726549909e-05\n",
      "Epoch Number :  90\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  7.456105959136039e-05\n",
      "Epoch Number :  91\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  6.705523992422968e-05\n",
      "Epoch Number :  92\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  6.070092786103487e-05\n",
      "Epoch Number :  93\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  5.462919580168091e-05\n",
      "Epoch Number :  94\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  4.9581562052480876e-05\n",
      "Epoch Number :  95\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  4.448887557373382e-05\n",
      "Epoch Number :  96\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  4.035646270494908e-05\n",
      "Epoch Number :  97\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  3.6591885873349383e-05\n",
      "Epoch Number :  98\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  3.285061757196672e-05\n",
      "Epoch Number :  99\n",
      "\t Training accuracy:  49.83364140480592\n",
      "\t Validation accuracy  47.22222222222222\n",
      "\t Test accuracy  51.0\n",
      "\t Epoch Loss  2.9845165045117028e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Train network \n",
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.PoissonNLLLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.SmoothL1Loss() #interesting ... but does not converge\n",
    "#criterion = nn.MSELoss() #0.83 but unstable\n",
    "\n",
    "if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "elif isinstance(criterion, nn.NLLLoss):\n",
    "    train_target = Variable(preprocessed_input_train_target)  # keep long tensors\n",
    "    validation_target = Variable(preprocessed_input_validation_target, volatile=True) # convert to float\n",
    "    test_target = Variable(test_target_100, volatile=True )\n",
    "    Noutputs = 2\n",
    "    \n",
    "else:\n",
    "    train_target = Variable(preprocessed_input_train_target.float()) # convert to float\n",
    "    validation_target = Variable(preprocessed_input_validation_target.float(), volatile=True ) # convert to float\n",
    "    test_target = Variable(test_target_100.float(), volatile=True )\n",
    "    Noutputs = 1\n",
    "        \n",
    "model = conv2DNet(Nchannels, Nsamples_100, Noutputs)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum=0.90, nesterov=False)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "#optimizer = optim.Adagrad(model.parameters())\n",
    "#optimizer = optim.Adamax(model.parameters())\n",
    "#optimizer = optim.ASGD(model.parameters())\n",
    "#optimizer = optim.RMSprop(model.parameters())\n",
    "#optimizer = optim.Rprop(model.parameters())\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True)\n",
    "\n",
    "batch_size = 15\n",
    "Nbatches = int(math.ceil(Ntrain/batch_size))\n",
    "Nepochs = 100\n",
    "Nrep = 1\n",
    "\n",
    "train_errors = torch.Tensor(Nepochs).zero_()\n",
    "test_errors = torch.Tensor(Nepochs).zero_()\n",
    "validation_errors = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "ep_loss = torch.Tensor(Nepochs).zero_()\n",
    "\n",
    "for i_rep in range(Nrep):\n",
    "    for i_ep in range(Nepochs):\n",
    "        for b_start in range(0, Ntrain, batch_size):\n",
    "            bsize_eff = batch_size - max(0, b_start+batch_size-Ntrain)  # boundary case\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            output = model(train_input.narrow(0, b_start, bsize_eff))\n",
    "            batch_loss = criterion(output, train_target.narrow(0, b_start, bsize_eff))            \n",
    "            ep_loss[i_ep] += batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step(ep_loss[i_ep])\n",
    "        \n",
    "        nb_train_errs = compute_nb_errors(model, train_input, train_target, batch_size)\n",
    "        nb_validation_errs = compute_nb_errors(model, validation_input, validation_target, batch_size)\n",
    "        nb_test_errs = compute_nb_errors(model, test_input, test_target, batch_size)\n",
    "        \n",
    "        print(\"Epoch Number : \", i_ep)\n",
    "        print(\"\\t Training accuracy: \", (100*(Ntrain-nb_train_errs)/Ntrain))\n",
    "        print(\"\\t Validation accuracy \",(100*(Nvalidation-nb_validation_errs)/Nvalidation)) \n",
    "        print(\"\\t Test accuracy \",(100*(Ntest-nb_test_errs)/Ntest))\n",
    "        \n",
    "        print(\"\\t Epoch Loss \", ep_loss[i_ep])\n",
    "        \n",
    "        train_errors[i_ep] = nb_train_errs\n",
    "        test_errors[i_ep] = nb_test_errs\n",
    "        validation_errors[i_ep] = nb_validation_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(train_errors))\n",
    "plt.plot(np.array(validation_errors))\n",
    "plt.plot(np.array(test_errors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(100*(Ntrain-np.array(train_errors))/Ntrain)\n",
    "plt.plot(100*(Nvalidation-np.array(validation_errors))/Nvalidation)\n",
    "plt.plot(100*(Ntest-np.array(test_errors))/Ntest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
